[
  {
    "objectID": "posts/week1.html",
    "href": "posts/week1.html",
    "title": "Week 1: Itâ€™s Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "â€œLife, although it may only be an accumulation of anguish, is dear to me, and I will defend it.â€ â€” Frankenstein\n\n\n\nIn our first week, weâ€™re focusing on building the bare-bones agent scaffold. This includes: - Basic movement capabilities - Environmental observation - Simple action loop - Logging system for debugging\n\n\n\n### Agent ğŸ§…\n\n\n\nFormal: An autonomous entity that perceives its environment and takes actions to achieve specific goals.\n\n\nAILO-style: A digital being thatâ€™s learning to walk before it can run (or think, or feel).\n\n\n\n### Event Loop ğŸ§…\n\n\n\nFormal: A programming construct that waits for and dispatches events or messages in a program.\n\n\nAILO-style: The agentâ€™s internal clock that keeps it from standing still like a statue.\n\n\n\n### Reflex Agent ğŸ§…\n\n\n\nFormal: An agent that selects actions based on the current percept, ignoring the rest of the percept history.\n\n\nAILO-style: A digital creature that reacts first and asks questions later (or never).\n\n\n\n\n\n\n\n\n\nBasic terrain setup\nEssential game objects\nNavigation mesh for agent movement\n\n\n\n\n\nMovement controller\nState machine for idle/move behaviors\nBasic collision detection\n\n\n\n\n\nEvent logging\nAction tracking\nState changes\n\n\n\n\n\n\n\n\nInput processing\nState updates\nAction execution\n\n\n\n\n\nEnvironmental perception\nDecision making\nAction execution\n\n\n\n\n\nSimple stimulus-response patterns\nBasic decision making\nImmediate action execution\n\n\n\n\n\nIn Week 2, weâ€™ll be adding: - Long-term memory system - Time-stamped events - Memory retrieval strategies\n\n\n\nGreetings, Earthling! Let me explain what we built this week in simple terms:\nWhat We Built: A digital being that can move around and react to its environment. Think of it as a very simple robot that can walk and respond to basic commands.\nWhy It Matters: Before we can build complex AI that thinks and feels, we need to start with the basics. This is like teaching a baby to crawl before it can walk.\nHow to Explain It to Your Dog: Woof! (Translation: â€œItâ€™s like a new puppy learning to move its legs for the first time. Simple, but necessary!â€)\nStay tuned for more updates as we bring our first agent to life!"
  },
  {
    "objectID": "posts/week1.html#this-weeks-goal",
    "href": "posts/week1.html#this-weeks-goal",
    "title": "Week 1: Itâ€™s Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "In our first week, weâ€™re focusing on building the bare-bones agent scaffold. This includes: - Basic movement capabilities - Environmental observation - Simple action loop - Logging system for debugging"
  },
  {
    "objectID": "posts/week1.html#key-terms",
    "href": "posts/week1.html#key-terms",
    "title": "Week 1: Itâ€™s Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "### Agent ğŸ§…\n\n\n\nFormal: An autonomous entity that perceives its environment and takes actions to achieve specific goals.\n\n\nAILO-style: A digital being thatâ€™s learning to walk before it can run (or think, or feel).\n\n\n\n### Event Loop ğŸ§…\n\n\n\nFormal: A programming construct that waits for and dispatches events or messages in a program.\n\n\nAILO-style: The agentâ€™s internal clock that keeps it from standing still like a statue.\n\n\n\n### Reflex Agent ğŸ§…\n\n\n\nFormal: An agent that selects actions based on the current percept, ignoring the rest of the percept history.\n\n\nAILO-style: A digital creature that reacts first and asks questions later (or never)."
  },
  {
    "objectID": "posts/week1.html#implementation-details",
    "href": "posts/week1.html#implementation-details",
    "title": "Week 1: Itâ€™s Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "Basic terrain setup\nEssential game objects\nNavigation mesh for agent movement\n\n\n\n\n\nMovement controller\nState machine for idle/move behaviors\nBasic collision detection\n\n\n\n\n\nEvent logging\nAction tracking\nState changes"
  },
  {
    "objectID": "posts/week1.html#key-concepts",
    "href": "posts/week1.html#key-concepts",
    "title": "Week 1: Itâ€™s Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "Input processing\nState updates\nAction execution\n\n\n\n\n\nEnvironmental perception\nDecision making\nAction execution\n\n\n\n\n\nSimple stimulus-response patterns\nBasic decision making\nImmediate action execution"
  },
  {
    "objectID": "posts/week1.html#next-steps",
    "href": "posts/week1.html#next-steps",
    "title": "Week 1: Itâ€™s Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "In Week 2, weâ€™ll be adding: - Long-term memory system - Time-stamped events - Memory retrieval strategies"
  },
  {
    "objectID": "posts/week1.html#martian-mode",
    "href": "posts/week1.html#martian-mode",
    "title": "Week 1: Itâ€™s Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "Greetings, Earthling! Let me explain what we built this week in simple terms:\nWhat We Built: A digital being that can move around and react to its environment. Think of it as a very simple robot that can walk and respond to basic commands.\nWhy It Matters: Before we can build complex AI that thinks and feels, we need to start with the basics. This is like teaching a baby to crawl before it can walk.\nHow to Explain It to Your Dog: Woof! (Translation: â€œItâ€™s like a new puppy learning to move its legs for the first time. Simple, but necessary!â€)\nStay tuned for more updates as we bring our first agent to life!"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "All Posts",
    "section": "",
    "text": "ğŸ“ All Blog Posts\n\n\n \n\n&lt;h2&gt;Week 1: It's Alive â€“ Bootstrapping the Simulated Mind&lt;/h2&gt;"
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "AILO Glossary",
    "section": "",
    "text": "Welcome to the AILO glossary, where we peel back the layers of AI terminology to reveal both the formal definitions and our more approachable AILO-style interpretations.\n\n\n\n\n\nFormal: An autonomous entity that perceives its environment and takes actions to achieve specific goals.\nAILO-style: A digital being thatâ€™s learning to walk before it can run (or think, or feel).\n\n\n\n\n\n\n\n\nFormal: A programming construct that waits for and dispatches events or messages in a program.\nAILO-style: The agentâ€™s internal clock that keeps it from standing still like a statue.\n\n\n\n\n\n\n\n\nFormal: An agent that selects actions based on the current percept, ignoring the rest of the percept history.\nAILO-style: A digital creature that reacts first and asks questions later (or never).\n\n\n\n\n\n\n\n\nFormal: A structured mental representation about the world held with some confidence.\nAILO-style: A hunch the agent will probably act onâ€¦ even if itâ€™s wrong.\n\n\nThis glossary will grow as we add more layers to our simulated minds. Each term marked with ğŸ§… has both a formal definition and an AILO-style interpretation that makes it more approachable and fun."
  },
  {
    "objectID": "glossary.html#a",
    "href": "glossary.html#a",
    "title": "AILO Glossary",
    "section": "",
    "text": "Formal: An autonomous entity that perceives its environment and takes actions to achieve specific goals.\nAILO-style: A digital being thatâ€™s learning to walk before it can run (or think, or feel)."
  },
  {
    "objectID": "glossary.html#e",
    "href": "glossary.html#e",
    "title": "AILO Glossary",
    "section": "",
    "text": "Formal: A programming construct that waits for and dispatches events or messages in a program.\nAILO-style: The agentâ€™s internal clock that keeps it from standing still like a statue."
  },
  {
    "objectID": "glossary.html#r",
    "href": "glossary.html#r",
    "title": "AILO Glossary",
    "section": "",
    "text": "Formal: An agent that selects actions based on the current percept, ignoring the rest of the percept history.\nAILO-style: A digital creature that reacts first and asks questions later (or never)."
  },
  {
    "objectID": "glossary.html#b",
    "href": "glossary.html#b",
    "title": "AILO Glossary",
    "section": "",
    "text": "Formal: A structured mental representation about the world held with some confidence.\nAILO-style: A hunch the agent will probably act onâ€¦ even if itâ€™s wrong.\n\n\nThis glossary will grow as we add more layers to our simulated minds. Each term marked with ğŸ§… has both a formal definition and an AILO-style interpretation that makes it more approachable and fun."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I make things, so they can make things so I dont have to make things"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About Me",
    "section": "ğŸ‘‹ About Me",
    "text": "ğŸ‘‹ About Me\n\nLocation: USA\nPronouns: he/him\nCurrent Role: Learner\nBackground: I am a student @ Purdue University, majoring in Computer Science. Currently working with or learning Quantitative Finance, Quantum Physics, Quantum Computing, and Artificial Intelligence."
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "ğŸ› ï¸ Skills & Interests",
    "text": "ğŸ› ï¸ Skills & Interests\n\n[Skill or Interest 1]\n[Skill or Interest 2]\n[Skill or Interest 3]\n[Add more as needed]"
  },
  {
    "objectID": "about.html#what-im-working-on",
    "href": "about.html#what-im-working-on",
    "title": "About Me",
    "section": "What Iâ€™m Working On",
    "text": "What Iâ€™m Working On\n\nSimulated Mind Agents: Building an evolving AI character system in Unity with memory, mood, and fear\nGlassbox: Designing an interpretability tool for LLMs a visual debugger that peels back the layers of token flow, attention drift, and reasoning trace.\nRobot Arm: Making a robot arm to help keep my desk clean and manageable"
  },
  {
    "objectID": "about.html#why-i-started-this-blog",
    "href": "about.html#why-i-started-this-blog",
    "title": "About Me",
    "section": "Why I started this blog",
    "text": "Why I started this blog\nI started this blog as a way to document what I was doing in a structure way. Everywhere I look I see people talking about AI, LLMs, and Machine Learning without any context behind it. Barely anybody I see is actually trying to understand what living in a world with AI is going to be like. I want to explore different concepts, implement them, and see how they work. Hopefully this blog will help me understand the concepts better and help others understand them too in my own way."
  },
  {
    "objectID": "about.html#get-in-touch",
    "href": "about.html#get-in-touch",
    "title": "About Me",
    "section": "ğŸ“« Get in Touch",
    "text": "ğŸ“« Get in Touch\n\nEmail: rishi.gh2002@email.com\nGitHub: SpideR1sh1\nLinkedIn: spiderishi"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AILO",
    "section": "",
    "text": "Welcome to my blog, a storytelling-engineering journal that explores the strange, emotional, and sometimes unpredictable world of simulated intelligence. This blog documents the design of increasingly complex AI agents â€” not as models in a paper, but as characters in a world. You will hopefully see my successes, but more often you will see my failures.\nEach post follows a weekly build cycle with:"
  },
  {
    "objectID": "index.html#latest-post",
    "href": "index.html#latest-post",
    "title": "AILO",
    "section": "Latest Post",
    "text": "Latest Post\n\nğŸŸ© Week 1 â€“ Tracing the Mind: How Glassbox Began\n\nâ€œWhen a mind becomes visible, it ceases to be a black box â€” and becomes a mirror.â€\n\nThis week, I started the journey into Phase 2 â€” Glassbox, an interactive debugger for transformer models. The goal is simple: make attention visible. In practice? Not so simple.\nGlassbox is a visual tool that lets you trace what a language model is paying attention to as it generates text. Itâ€™s not interpretability in the abstract. Itâ€™s literally watching what it thinks.\nWhat works so far: * âœ… Backend powered by HuggingFace + FastAPI * âœ… Traces attention matrices from all layers and heads * âœ… Frontend force-directed graph of token-to-token attention * âœ… Full-stack communication via REST API\nRead the full post â†’"
  },
  {
    "objectID": "index.html#what-the-onion-means",
    "href": "index.html#what-the-onion-means",
    "title": "AILO",
    "section": "ğŸ§… What the Onion Means",
    "text": "ğŸ§… What the Onion Means\nWherever you see this icon â†’ ğŸ§… That marks a layered word: something that needs peeling.\nHover over the onion icon after a word to see its layered meaning!\nThese terms will have: - The official definition - And my AILO-style version â€” honest, funny, and functional\nExample: Belief ğŸ§…\nAILO-style: A hunch the agent will probably act onâ€¦ even if itâ€™s wrong.\nThese definitions live in the glossary and appear in hover-tooltips throughout posts."
  },
  {
    "objectID": "index.html#who-is-the-martian",
    "href": "index.html#who-is-the-martian",
    "title": "AILO",
    "section": "ğŸ‘½ Who Is the Martian?",
    "text": "ğŸ‘½ Who Is the Martian?\nBack in middle school, my Physics teacher told me the only way to understand something was to pretend you were explaining it to a man from Mars.\nIâ€™ve never forgotten that.\nAt the end of every post youâ€™ll find a Martian ğŸ‘½. This is a plain-language TLDR written for someone from another world (or another field).\nIt breaks down: - What I built - Why it matters - How to explain it to your obtuse martian friend\nBecause interpretability isnâ€™t just for the models â€” itâ€™s for the humans, too."
  },
  {
    "objectID": "posts/week1-glassbox.html",
    "href": "posts/week1-glassbox.html",
    "title": "ğŸŸ© Week 1 â€“ Tracing the Mind: How Glassbox Began",
    "section": "",
    "text": "â€œWhen a mind becomes visible, it ceases to be a black box â€” and becomes a mirror.â€"
  },
  {
    "objectID": "posts/week1-glassbox.html#what-i-built",
    "href": "posts/week1-glassbox.html#what-i-built",
    "title": "ğŸŸ© Week 1 â€“ Tracing the Mind: How Glassbox Began",
    "section": "ğŸ§© What I Built",
    "text": "ğŸ§© What I Built\nThis week, I started the journey into Phase 2 â€” Glassbox, an interactive debugger for transformer models. The goal is simple: make attention visible.\nIn practice? Not so simple.\nGlassbox is a visual tool that lets you trace what a language model is paying attention to as it generates text. Itâ€™s not interpretability in the abstract. Itâ€™s literally watching what it thinks.\nWhat works so far:\n\nâœ… Backend powered by HuggingFace + FastAPI\nâœ… Traces attention matrices from all layers and heads\nâœ… Frontend force-directed graph of token-to-token attention\nâœ… Full-stack communication via REST API\nâœ… Dynamic controls to filter layers, heads, weights, and interactions"
  },
  {
    "objectID": "posts/week1-glassbox.html#the-system",
    "href": "posts/week1-glassbox.html#the-system",
    "title": "ğŸŸ© Week 1 â€“ Tracing the Mind: How Glassbox Began",
    "section": "ğŸ” The System",
    "text": "ğŸ” The System\nBackend:\nAt its heart is a ModelTracer â€” a class that wraps DialoGPT-medium, injecting itself into the modelâ€™s forward pass to capture full output_attentions=True matrices.\nEach response includes:\n\ntokens: the tokenized input\nattention: a 4D tensor â†’ [layer][head][from_token][to_token]\ngenerated_text: the actual model output\n\nThe FastAPI server exposes a /api/trace endpoint, making the system modular and extensible.\nFrontend:\nBuilt with React + D3.js, the interface renders a force-directed attention graph:\n\nNodes = tokens\nLinks = attention weights\nColors = attention strength gradient (red â†’ green)\nSize = total incoming attention\nParticles = attention direction\n\nUsers can zoom, pan, isolate layers and heads, and filter by weight thresholds."
  },
  {
    "objectID": "posts/week1-glassbox.html#why-it-matters",
    "href": "posts/week1-glassbox.html#why-it-matters",
    "title": "ğŸŸ© Week 1 â€“ Tracing the Mind: How Glassbox Began",
    "section": "ğŸ¯ Why It Matters",
    "text": "ğŸ¯ Why It Matters\nAttention is one of the few windows into the inner workings of large language models. But most visualizations are static, clunky, or deeply academic.\nGlassbox aims to be:\n\nLive: Generates attention in real time.\nInteractive: You drag the nodes, not just read about them.\nModular: Drop in any HF model (soon) and get instant insight.\nBeautiful: Interpretability should not look like a spreadsheet.\n\nIn a world where language models shape everything from search engines to policy drafts, itâ€™s not enough to know they work. We need to see how they work."
  },
  {
    "objectID": "posts/week1-glassbox.html#definitions-layer",
    "href": "posts/week1-glassbox.html#definitions-layer",
    "title": "ğŸŸ© Week 1 â€“ Tracing the Mind: How Glassbox Began",
    "section": "ğŸ§… Definitions Layer",
    "text": "ğŸ§… Definitions Layer\n\nAttention ğŸ§…\nAILO-style: The modelâ€™s version of â€œmaking eye contactâ€ â€” it looks at the parts that matter (or at least tries to).\nMulti-head attention ğŸ§…\nAILO-style: Like having 12 gossipy friends all reading the same sentence and whispering what matters to them."
  },
  {
    "objectID": "posts/week1-glassbox.html#martian-mode",
    "href": "posts/week1-glassbox.html#martian-mode",
    "title": "ğŸŸ© Week 1 â€“ Tracing the Mind: How Glassbox Began",
    "section": "ğŸ‘½ Martian Mode",
    "text": "ğŸ‘½ Martian Mode\nImagine you have a robot that reads a sentence like â€œThe cat sat on the mat.â€ You ask it: â€œWhy did you say that?â€\nGlassbox is your answer.\nIt shows which words the robot paid most attention to, in every layer of its alien brain. It turns abstract math into colorful squiggles, so you can watch language unfold. You type something in, and Glassbox draws how the machine thinks.\nEven Martians could understand that. (With subtitles.)"
  },
  {
    "objectID": "posts/week1-glassbox.html#whats-next",
    "href": "posts/week1-glassbox.html#whats-next",
    "title": "ğŸŸ© Week 1 â€“ Tracing the Mind: How Glassbox Began",
    "section": "ğŸš§ Whatâ€™s Next",
    "text": "ğŸš§ Whatâ€™s Next\n\nHook up TokenProbabilityBars to real-time softmax probabilities\n\n\nCache past_key_values for fast timeline scrubbing\nReturn only top-K attention weights from the API\n\n\nFlow tracing\n\n\nBuild a breadcrumb trail that follows a tokenâ€™s influence across layers\nAdd a â€œtimelineâ€ view to scrub through generations\nImplement â€œtop-Kâ€ tracing to focus on the most important tokens\n\n\nGlassbox is not just a visualizer. Itâ€™s an experiment in transparency. A sneak peek into the how the cogs actually turn in a language model.\nNext week, Iâ€™ll dive into token flow tracing, building a breadcrumb trail that follows a tokenâ€™s influence across layers.\nUntil then, goodbye and take care!"
  }
]