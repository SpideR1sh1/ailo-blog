[
  {
    "objectID": "posts/week1.html",
    "href": "posts/week1.html",
    "title": "Week 1: It‚Äôs Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "‚ÄúLife, although it may only be an accumulation of anguish, is dear to me, and I will defend it.‚Äù ‚Äî Frankenstein\n\n\n\nIn our first week, we‚Äôre focusing on building the bare-bones agent scaffold. This includes: - Basic movement capabilities - Environmental observation - Simple action loop - Logging system for debugging\n\n\n\n### Agent üßÖ\n\n\n\nFormal: An autonomous entity that perceives its environment and takes actions to achieve specific goals.\n\n\nAILO-style: A digital being that‚Äôs learning to walk before it can run (or think, or feel).\n\n\n\n### Event Loop üßÖ\n\n\n\nFormal: A programming construct that waits for and dispatches events or messages in a program.\n\n\nAILO-style: The agent‚Äôs internal clock that keeps it from standing still like a statue.\n\n\n\n### Reflex Agent üßÖ\n\n\n\nFormal: An agent that selects actions based on the current percept, ignoring the rest of the percept history.\n\n\nAILO-style: A digital creature that reacts first and asks questions later (or never).\n\n\n\n\n\n\n\n\n\nBasic terrain setup\nEssential game objects\nNavigation mesh for agent movement\n\n\n\n\n\nMovement controller\nState machine for idle/move behaviors\nBasic collision detection\n\n\n\n\n\nEvent logging\nAction tracking\nState changes\n\n\n\n\n\n\n\n\nInput processing\nState updates\nAction execution\n\n\n\n\n\nEnvironmental perception\nDecision making\nAction execution\n\n\n\n\n\nSimple stimulus-response patterns\nBasic decision making\nImmediate action execution\n\n\n\n\n\nIn Week 2, we‚Äôll be adding: - Long-term memory system - Time-stamped events - Memory retrieval strategies\n\n\n\nGreetings, Earthling! Let me explain what we built this week in simple terms:\nWhat We Built: A digital being that can move around and react to its environment. Think of it as a very simple robot that can walk and respond to basic commands.\nWhy It Matters: Before we can build complex AI that thinks and feels, we need to start with the basics. This is like teaching a baby to crawl before it can walk.\nHow to Explain It to Your Dog: Woof! (Translation: ‚ÄúIt‚Äôs like a new puppy learning to move its legs for the first time. Simple, but necessary!‚Äù)\nStay tuned for more updates as we bring our first agent to life!"
  },
  {
    "objectID": "posts/week1.html#this-weeks-goal",
    "href": "posts/week1.html#this-weeks-goal",
    "title": "Week 1: It‚Äôs Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "In our first week, we‚Äôre focusing on building the bare-bones agent scaffold. This includes: - Basic movement capabilities - Environmental observation - Simple action loop - Logging system for debugging"
  },
  {
    "objectID": "posts/week1.html#key-terms",
    "href": "posts/week1.html#key-terms",
    "title": "Week 1: It‚Äôs Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "### Agent üßÖ\n\n\n\nFormal: An autonomous entity that perceives its environment and takes actions to achieve specific goals.\n\n\nAILO-style: A digital being that‚Äôs learning to walk before it can run (or think, or feel).\n\n\n\n### Event Loop üßÖ\n\n\n\nFormal: A programming construct that waits for and dispatches events or messages in a program.\n\n\nAILO-style: The agent‚Äôs internal clock that keeps it from standing still like a statue.\n\n\n\n### Reflex Agent üßÖ\n\n\n\nFormal: An agent that selects actions based on the current percept, ignoring the rest of the percept history.\n\n\nAILO-style: A digital creature that reacts first and asks questions later (or never)."
  },
  {
    "objectID": "posts/week1.html#implementation-details",
    "href": "posts/week1.html#implementation-details",
    "title": "Week 1: It‚Äôs Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "Basic terrain setup\nEssential game objects\nNavigation mesh for agent movement\n\n\n\n\n\nMovement controller\nState machine for idle/move behaviors\nBasic collision detection\n\n\n\n\n\nEvent logging\nAction tracking\nState changes"
  },
  {
    "objectID": "posts/week1.html#key-concepts",
    "href": "posts/week1.html#key-concepts",
    "title": "Week 1: It‚Äôs Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "Input processing\nState updates\nAction execution\n\n\n\n\n\nEnvironmental perception\nDecision making\nAction execution\n\n\n\n\n\nSimple stimulus-response patterns\nBasic decision making\nImmediate action execution"
  },
  {
    "objectID": "posts/week1.html#next-steps",
    "href": "posts/week1.html#next-steps",
    "title": "Week 1: It‚Äôs Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "In Week 2, we‚Äôll be adding: - Long-term memory system - Time-stamped events - Memory retrieval strategies"
  },
  {
    "objectID": "posts/week1.html#martian-mode",
    "href": "posts/week1.html#martian-mode",
    "title": "Week 1: It‚Äôs Alive - Bootstrapping the Simulated Mind",
    "section": "",
    "text": "Greetings, Earthling! Let me explain what we built this week in simple terms:\nWhat We Built: A digital being that can move around and react to its environment. Think of it as a very simple robot that can walk and respond to basic commands.\nWhy It Matters: Before we can build complex AI that thinks and feels, we need to start with the basics. This is like teaching a baby to crawl before it can walk.\nHow to Explain It to Your Dog: Woof! (Translation: ‚ÄúIt‚Äôs like a new puppy learning to move its legs for the first time. Simple, but necessary!‚Äù)\nStay tuned for more updates as we bring our first agent to life!"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "All Posts",
    "section": "",
    "text": "üìù All Blog Posts\n\n\n \n\n&lt;h2&gt;Week 1: It's Alive ‚Äì Bootstrapping the Simulated Mind&lt;/h2&gt;"
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "AILO Glossary",
    "section": "",
    "text": "Welcome to the AILO glossary, where we peel back the layers of AI terminology to reveal both the formal definitions and our more approachable AILO-style interpretations.\n\n\n\n\n\nFormal: An autonomous entity that perceives its environment and takes actions to achieve specific goals.\nAILO-style: A digital being that‚Äôs learning to walk before it can run (or think, or feel).\n\n\n\n\n\n\n\n\nFormal: A programming construct that waits for and dispatches events or messages in a program.\nAILO-style: The agent‚Äôs internal clock that keeps it from standing still like a statue.\n\n\n\n\n\n\n\n\nFormal: An agent that selects actions based on the current percept, ignoring the rest of the percept history.\nAILO-style: A digital creature that reacts first and asks questions later (or never).\n\n\n\n\n\n\n\n\nFormal: A structured mental representation about the world held with some confidence.\nAILO-style: A hunch the agent will probably act on‚Ä¶ even if it‚Äôs wrong.\n\n\nThis glossary will grow as we add more layers to our simulated minds. Each term marked with üßÖ has both a formal definition and an AILO-style interpretation that makes it more approachable and fun."
  },
  {
    "objectID": "glossary.html#a",
    "href": "glossary.html#a",
    "title": "AILO Glossary",
    "section": "",
    "text": "Formal: An autonomous entity that perceives its environment and takes actions to achieve specific goals.\nAILO-style: A digital being that‚Äôs learning to walk before it can run (or think, or feel)."
  },
  {
    "objectID": "glossary.html#e",
    "href": "glossary.html#e",
    "title": "AILO Glossary",
    "section": "",
    "text": "Formal: A programming construct that waits for and dispatches events or messages in a program.\nAILO-style: The agent‚Äôs internal clock that keeps it from standing still like a statue."
  },
  {
    "objectID": "glossary.html#r",
    "href": "glossary.html#r",
    "title": "AILO Glossary",
    "section": "",
    "text": "Formal: An agent that selects actions based on the current percept, ignoring the rest of the percept history.\nAILO-style: A digital creature that reacts first and asks questions later (or never)."
  },
  {
    "objectID": "glossary.html#b",
    "href": "glossary.html#b",
    "title": "AILO Glossary",
    "section": "",
    "text": "Formal: A structured mental representation about the world held with some confidence.\nAILO-style: A hunch the agent will probably act on‚Ä¶ even if it‚Äôs wrong.\n\n\nThis glossary will grow as we add more layers to our simulated minds. Each term marked with üßÖ has both a formal definition and an AILO-style interpretation that makes it more approachable and fun."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I make things, so they can make things so I dont have to make things"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About Me",
    "section": "üëã About Me",
    "text": "üëã About Me\n\nLocation: USA\nPronouns: he/him\nCurrent Role: Learner\nBackground: I am a student @ Purdue University, majoring in Computer Science. Currently working with or learning Quantitative Finance, Quantum Physics, Quantum Computing, and Artificial Intelligence."
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "üõ†Ô∏è Skills & Interests",
    "text": "üõ†Ô∏è Skills & Interests\n\n[Skill or Interest 1]\n[Skill or Interest 2]\n[Skill or Interest 3]\n[Add more as needed]"
  },
  {
    "objectID": "about.html#what-im-working-on",
    "href": "about.html#what-im-working-on",
    "title": "About Me",
    "section": "What I‚Äôm Working On",
    "text": "What I‚Äôm Working On\n\nSimulated Mind Agents: Building an evolving AI character system in Unity with memory, mood, and fear\nGlassbox: Designing an interpretability tool for LLMs a visual debugger that peels back the layers of token flow, attention drift, and reasoning trace.\nRobot Arm: Making a robot arm to help keep my desk clean and manageable"
  },
  {
    "objectID": "about.html#why-i-started-this-blog",
    "href": "about.html#why-i-started-this-blog",
    "title": "About Me",
    "section": "Why I started this blog",
    "text": "Why I started this blog\nI started this blog as a way to document what I was doing in a structure way. Everywhere I look I see people talking about AI, LLMs, and Machine Learning without any context behind it. Barely anybody I see is actually trying to understand what living in a world with AI is going to be like. I want to explore different concepts, implement them, and see how they work. Hopefully this blog will help me understand the concepts better and help others understand them too in my own way."
  },
  {
    "objectID": "about.html#get-in-touch",
    "href": "about.html#get-in-touch",
    "title": "About Me",
    "section": "üì´ Get in Touch",
    "text": "üì´ Get in Touch\n\nEmail: rishi.gh2002@email.com\nGitHub: SpideR1sh1\nLinkedIn: spiderishi"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AILO",
    "section": "",
    "text": "Welcome to my blog, a storytelling-engineering journal that explores the strange, emotional, and sometimes unpredictable world of simulated intelligence. This blog documents the design of increasingly complex AI agents ‚Äî not as models in a paper, but as characters in a world. You will hopefully see my successes, but more often you will see my failures.\nEach post follows a weekly build cycle with:"
  },
  {
    "objectID": "index.html#latest-post",
    "href": "index.html#latest-post",
    "title": "AILO",
    "section": "Latest Post",
    "text": "Latest Post\n\nüü© Week 1 ‚Äì Tracing the Mind: How Glassbox Began\n\n‚ÄúWhen a mind becomes visible, it ceases to be a black box ‚Äî and becomes a mirror.‚Äù\n\nThis week, I started the journey into Phase 2 ‚Äî Glassbox, an interactive debugger for transformer models. The goal is simple: make attention visible. In practice? Not so simple.\nGlassbox is a visual tool that lets you trace what a language model is paying attention to as it generates text. It‚Äôs not interpretability in the abstract. It‚Äôs literally watching what it thinks.\nWhat works so far: * ‚úÖ Backend powered by HuggingFace + FastAPI * ‚úÖ Traces attention matrices from all layers and heads * ‚úÖ Frontend force-directed graph of token-to-token attention * ‚úÖ Full-stack communication via REST API\nRead the full post ‚Üí"
  },
  {
    "objectID": "index.html#what-the-onion-means",
    "href": "index.html#what-the-onion-means",
    "title": "AILO",
    "section": "üßÖ What the Onion Means",
    "text": "üßÖ What the Onion Means\nWherever you see this icon ‚Üí üßÖ That marks a layered word: something that needs peeling.\nHover over the onion icon after a word to see its layered meaning!\nThese terms will have: - The official definition - And my AILO-style version ‚Äî honest, funny, and functional\nExample: Belief üßÖ\nAILO-style: A hunch the agent will probably act on‚Ä¶ even if it‚Äôs wrong.\nThese definitions live in the glossary and appear in hover-tooltips throughout posts."
  },
  {
    "objectID": "index.html#who-is-the-martian",
    "href": "index.html#who-is-the-martian",
    "title": "AILO",
    "section": "üëΩ Who Is the Martian?",
    "text": "üëΩ Who Is the Martian?\nBack in middle school, my Physics teacher told me the only way to understand something was to pretend you were explaining it to a man from Mars.\nI‚Äôve never forgotten that.\nAt the end of every post you‚Äôll find a Martian üëΩ. This is a plain-language TLDR written for someone from another world (or another field).\nIt breaks down: - What I built - Why it matters - How to explain it to your obtuse martian friend\nBecause interpretability isn‚Äôt just for the models ‚Äî it‚Äôs for the humans, too."
  },
  {
    "objectID": "posts/week1-glassbox.html",
    "href": "posts/week1-glassbox.html",
    "title": "üü© Week 1 ‚Äì Tracing the Mind: How Glassbox Began",
    "section": "",
    "text": "‚ÄúWhen a mind becomes visible, it ceases to be a black box ‚Äî and becomes a mirror.‚Äù"
  },
  {
    "objectID": "posts/week1-glassbox.html#what-i-built",
    "href": "posts/week1-glassbox.html#what-i-built",
    "title": "üü© Week 1 ‚Äì Tracing the Mind: How Glassbox Began",
    "section": "üß© What I Built",
    "text": "üß© What I Built\nThis week, I started the journey into Phase 2 ‚Äî Glassbox, an interactive debugger for transformer models. The goal is simple: make attention visible.\nIn practice? Not so simple.\nGlassbox is a visual tool that lets you trace what a language model is paying attention to as it generates text. It‚Äôs not interpretability in the abstract. It‚Äôs literally watching what it thinks.\nWhat works so far:\n\n‚úÖ Backend powered by HuggingFace + FastAPI\n‚úÖ Traces attention matrices from all layers and heads\n‚úÖ Frontend force-directed graph of token-to-token attention\n‚úÖ Full-stack communication via REST API\n‚úÖ Dynamic controls to filter layers, heads, weights, and interactions"
  },
  {
    "objectID": "posts/week1-glassbox.html#the-system",
    "href": "posts/week1-glassbox.html#the-system",
    "title": "üü© Week 1 ‚Äì Tracing the Mind: How Glassbox Began",
    "section": "üîç The System",
    "text": "üîç The System\nBackend:\nAt its heart is a ModelTracer ‚Äî a class that wraps DialoGPT-medium, injecting itself into the model‚Äôs forward pass to capture full output_attentions=True matrices.\nEach response includes:\n\ntokens: the tokenized input\nattention: a 4D tensor ‚Üí [layer][head][from_token][to_token]\ngenerated_text: the actual model output\n\nThe FastAPI server exposes a /api/trace endpoint, making the system modular and extensible.\nFrontend:\nBuilt with React + D3.js, the interface renders a force-directed attention graph:\n\nNodes = tokens\nLinks = attention weights\nColors = attention strength gradient (red ‚Üí green)\nSize = total incoming attention\nParticles = attention direction\n\nUsers can zoom, pan, isolate layers and heads, and filter by weight thresholds."
  },
  {
    "objectID": "posts/week1-glassbox.html#why-it-matters",
    "href": "posts/week1-glassbox.html#why-it-matters",
    "title": "üü© Week 1 ‚Äì Tracing the Mind: How Glassbox Began",
    "section": "üéØ Why It Matters",
    "text": "üéØ Why It Matters\nAttention is one of the few windows into the inner workings of large language models. But most visualizations are static, clunky, or deeply academic.\nGlassbox aims to be:\n\nLive: Generates attention in real time.\nInteractive: You drag the nodes, not just read about them.\nModular: Drop in any HF model (soon) and get instant insight.\nBeautiful: Interpretability should not look like a spreadsheet.\n\nIn a world where language models shape everything from search engines to policy drafts, it‚Äôs not enough to know they work. We need to see how they work."
  },
  {
    "objectID": "posts/week1-glassbox.html#definitions-layer",
    "href": "posts/week1-glassbox.html#definitions-layer",
    "title": "üü© Week 1 ‚Äì Tracing the Mind: How Glassbox Began",
    "section": "üßÖ Definitions Layer",
    "text": "üßÖ Definitions Layer\n\nAttention üßÖ\nAILO-style: The model‚Äôs version of ‚Äúmaking eye contact‚Äù ‚Äî it looks at the parts that matter (or at least tries to).\nMulti-head attention üßÖ\nAILO-style: Like having 12 gossipy friends all reading the same sentence and whispering what matters to them."
  },
  {
    "objectID": "posts/week1-glassbox.html#martian-mode",
    "href": "posts/week1-glassbox.html#martian-mode",
    "title": "üü© Week 1 ‚Äì Tracing the Mind: How Glassbox Began",
    "section": "üëΩ Martian Mode",
    "text": "üëΩ Martian Mode\nImagine you have a robot that reads a sentence like ‚ÄúThe cat sat on the mat.‚Äù You ask it: ‚ÄúWhy did you say that?‚Äù\nGlassbox is your answer.\nIt shows which words the robot paid most attention to, in every layer of its alien brain. It turns abstract math into colorful squiggles, so you can watch language unfold. You type something in, and Glassbox draws how the machine thinks.\nEven Martians could understand that. (With subtitles.)"
  },
  {
    "objectID": "posts/week1-glassbox.html#whats-next",
    "href": "posts/week1-glassbox.html#whats-next",
    "title": "üü© Week 1 ‚Äì Tracing the Mind: How Glassbox Began",
    "section": "üöß What‚Äôs Next",
    "text": "üöß What‚Äôs Next\n\nHook up TokenProbabilityBars to real-time softmax probabilities\n\n\nCache past_key_values for fast timeline scrubbing\nReturn only top-K attention weights from the API\n\n\nFlow tracing\n\n\nBuild a breadcrumb trail that follows a token‚Äôs influence across layers\nAdd a ‚Äútimeline‚Äù view to scrub through generations\nImplement ‚Äútop-K‚Äù tracing to focus on the most important tokens\n\n\nGlassbox is not just a visualizer. It‚Äôs an experiment in transparency. A sneak peek into the how the cogs actually turn in a language model.\nNext week, I‚Äôll dive into token flow tracing, building a breadcrumb trail that follows a token‚Äôs influence across layers.\nUntil then, goodbye and take care!"
  }
]